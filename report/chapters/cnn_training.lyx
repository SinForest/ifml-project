#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Training a Neural Network
\end_layout

\begin_layout Standard
This sub-project has its focus on training a convolutional neural network
 on detecting the genres of movies by their respective posters.
\end_layout

\begin_layout Subsection
Prerequisites
\end_layout

\begin_layout Standard
The whole training pipeline is written in Python 3, using pytorch and torchvisio
n for defining the network architecture, preprocessing the data and finally
 train the model.
 Also, NumPy was used for some matrix handling outside pytorch, matplotlib
 for plotting the results and tqdm for nicer verbosity.
\end_layout

\begin_layout Standard
The training process was running on a GeForce GTX 1070 and a Intel Core
 i5-3570K, operating via Arch Linux.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Output-+-Metrics"

\end_inset

Output + Metrics
\end_layout

\begin_layout Standard
Since all posters in out dataset contain between 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula $7$
\end_inset

 labels, that have no order or weight, why we could not simplify the task
 to something easier than a multi-label classification problem.
 Therefore, our model outputs 
\begin_inset Formula $N=23$
\end_inset

 independent probabilities, each one representing how sure the model is,
 that the input (eventually among others) belongs to the respective genre.
\end_layout

\begin_layout Standard
To measure the performance of our model, two different metrics are used.
 First, we use a binary cross entropy loss, which is minimized during training.
 For a prediction vector 
\begin_inset Formula $p\in\left[0,1\right]^{N}$
\end_inset

 and a target vector 
\begin_inset Formula $t\in\left\{ 0,1\right\} ^{N}$
\end_inset

, the BCE loss is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L\left(x,y\right)=\frac{1}{N}\sum_{n=1}^{N}\left[t_{i}\cdot\log p_{i}+\left(1-t_{i}\right)\log\left(1-p_{i}\right)\right]\text{,}
\]

\end_inset

which rewards high probabilities on correct labels and penalizes high probabilit
ies on wrong labels.
\end_layout

\begin_layout Standard
To finally evaluate our models on the test set, we came up with an easy
 accuracy measure for multi-label classification like ours.
 If the 
\begin_inset Formula $M$
\end_inset

 labels assigned to a sample are in the Top
\begin_inset Formula $M$
\end_inset

 predictions (label-predictions with the highest probabilities), we want
 an accuracy of 
\begin_inset Formula $1$
\end_inset

.
 In contrast, we want an accuracy of 
\begin_inset Formula $0$
\end_inset

, if they are in the Bot
\begin_inset Formula $M$
\end_inset

(label-predictions with the worst probabilities).
 So we take every ground truth label separately, score it with 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

 in those two cases and scale linear between them.
 Then, the score of all labels is averaged.
 Example:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $p_{\Xi}=\left(\begin{array}{c}
0.7\\
0.2\\
0.1\\
0.9\\
0.5
\end{array}\right)\text{, }t_{\Xi}=\left(\begin{array}{c}
1\\
0\\
0\\
0\\
1
\end{array}\right)\text{, scores}_{\Xi}=\left[1,0.5\right]\text{, acc}_{\Xi}=0.75$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $p_{\heartsuit}=\left(\begin{array}{c}
0.9\\
0.1\\
0.5\\
0.8\\
0.2
\end{array}\right)\text{, }t_{\heartsuit}=\left(\begin{array}{c}
0\\
0\\
0\\
0\\
1
\end{array}\right)\text{, scores}_{\heartsuit}=\left[0.25\right]\text{, acc}_{\heartsuit}=0.25$
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In the 
\begin_inset Formula $\Xi$
\end_inset

-example, two labels are assigned, class 
\begin_inset Formula $0$
\end_inset

 and class 
\begin_inset Formula $4$
\end_inset

.
 Class 
\begin_inset Formula $0$
\end_inset

 is rated 2nd (which is in the Top
\begin_inset Formula $2$
\end_inset

), therefore it has an accuracy of 
\begin_inset Formula $1$
\end_inset

.
 Class 
\begin_inset Formula $4$
\end_inset

 is rated 3rd and therefore exactly in the middle of the linear scaled area.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\heartsuit$
\end_inset

-example shows the linear scaling better.
 Since there is only one label, the 2nd, 3rd and 4th predictions score 
\begin_inset Formula $0.75$
\end_inset

, 
\begin_inset Formula $0.5$
\end_inset

 and 
\begin_inset Formula $0.25$
\end_inset

.
\end_layout

\begin_layout Subsection
Approaches
\end_layout

\begin_layout Standard
On the way of finding a good network architecture for our problem, many
 improvements have been made to the architecture definition, as well as
 the training process.
 The following sub-chapters describe milestones on this way.
\end_layout

\begin_layout Subsubsection
Small Network
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/cobalt/vl/ifml/project/cnn_training/plot2nd.png
	lyxscale 15
	width 48col%

\end_inset


\begin_inset space \space{}
\end_inset


\begin_inset Graphics
	filename /home/cobalt/vl/ifml/project/cnn_training/plot3rd.png
	lyxscale 15
	width 48col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:2nd-3rd"

\end_inset

Losses on the train set (in train mode) and val set (in eval mode).

\series bold
 
\begin_inset Newline newline
\end_inset

left:
\series default
 The small network architecture in 
\begin_inset Formula $\sim250$
\end_inset

 epochs of training.
 
\begin_inset Newline newline
\end_inset


\series bold
right:
\series default
 The smaller network in 
\begin_inset Formula $\sim200$
\end_inset

 epochs of training.
\begin_inset Newline newline
\end_inset


\series bold
both:
\series default
 The triangles indicate a reduction of the learning rate.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The small architecture is inspired by VGG 
\begin_inset Note Note
status open

\begin_layout Plain Layout
ref
\end_layout

\end_inset

, especially its convolutional layers.
 Instead of big filters, multiple convolutional layers with small filters
 are used before each pooling layer.
\end_layout

\begin_layout Standard
As non-linear activations, this network uses ELUs.
 Each convolutional layer is prefixed with a batch normalization layer and
 each block of two convolutional layers is followed by a 
\begin_inset Formula $2\times2$
\end_inset

 max-pooling and a dropout layer with 
\begin_inset Formula $p=0.25$
\end_inset

.
 The fully connected layers are prefixed with a dropout layer with 
\begin_inset Formula $p=0.5$
\end_inset

.
\end_layout

\begin_layout Standard
As loss function, we use the BCELoss as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Output-+-Metrics"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we optimize our network with Adam
\begin_inset Note Note
status open

\begin_layout Plain Layout
ref
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
To improve learning, a learning rate scheduler has been used, that reduces
 the learning rate if the validation loss has not improved for several epochs.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
As to be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2nd-3rd"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (l), the model tends to overfit.
 This is mainly indicated by the training loss constantly falling until
 the 
\begin_inset Formula $100$
\end_inset

th epoch, while the validation loss raises again after dropping a little.
 Since the losses achieved are only marginally better then the losses of
 the first epoch, the model does not seem to learn relevant information
 about the data.
\end_layout

\begin_layout Subsubsection
Smaller Network
\end_layout

\begin_layout Standard
To ensure that the bad losses do not come from the model being too big for
 the problem and therefore underfitting, a smaller network architecture
 is tested.
 It is trained the exact same way, but is inspired more by AlexNet, having
 single convolutional layers with bigger filters and therefore less then
 half the number of parameters.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2nd-3rd"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (r) shows that the validation loss stayed nearly the same, compared to
 the last experiment, while the training loss converges similar, but to
 a higher bound.
 We now assume that the 
\begin_inset Quotes eld
\end_inset

Small
\begin_inset Quotes erd
\end_inset

 model is not too big for our problem, so the overfitting might come from
 too less data.
\end_layout

\begin_layout Subsubsection
Small Network (random crop)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename /home/cobalt/vl/ifml/project/cnn_training/plot4th.png
	lyxscale 15
	width 48col%

\end_inset


\begin_inset space \space{}
\end_inset


\begin_inset Graphics
	filename /home/cobalt/vl/ifml/project/cnn_training/plot5th.png
	lyxscale 15
	width 48col%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:4th-5th"

\end_inset

Losses on the train set (in train mode) and val set (in eval mode).

\series bold
 
\begin_inset Newline newline
\end_inset

left:
\series default
 The small network architecture in 
\begin_inset Formula $\sim250$
\end_inset

 epochs of training, using random crops in training and ten crops in validation.
 
\begin_inset Newline newline
\end_inset


\series bold
right:
\series default
 The midsized network in 
\begin_inset Formula $\sim220$
\end_inset

 epochs of training, using the same crops.
\begin_inset Newline newline
\end_inset


\series bold
both:
\series default
 The triangles indicate a reduction of the learning rate.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this milestone, the first architecture has been trained again, but this
 time on random crops of the data, to counter the problem of the too small
 training set.
 At training time, the model gets a random 
\begin_inset Formula $160\times160$
\end_inset

 crop of each (augmented) image as input, at test time it gets the center
 crop and the four corner crops of the images as well as their horizontally
 mirrored versions and averages over the results.
\end_layout

\begin_layout Standard
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Standard
This time, as shown in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:4th-5th"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (l), both training loss and validation loss converge together to a bound
 that is lower then the validation loss has been in all experiments before,
 which might be a partial success.
 It is notable, that the validation loss is always lower than the training
 loss, which should not happen in normal cases.
 But this might come from the averaging over the results, bringing an advantage
 similar to those of ensemble methods.
\end_layout

\begin_layout Subsubsection
Medium Network (random crop)
\end_layout

\begin_layout Standard
This is the same training process as before, but with a bigger network architect
ure, which has more feature maps per convolutional layer and more neurons
 in the first fully connected layer.
 Also, the learning rate scheduler as been weakened, since it seemed like
 it might induce convergence too soon by making the learning rate too small.
\end_layout

\begin_layout Subsection
Baselines
\end_layout

\begin_layout Standard
Random Neural Network
\end_layout

\begin_layout Standard
Distribution
\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\end_body
\end_document
