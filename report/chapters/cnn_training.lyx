#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Training a Neural Network
\end_layout

\begin_layout Standard
This sub-project has its focus on training a convolutional neural network
 on detecting the genres of movies by their respective posters.
\end_layout

\begin_layout Subsection
Prerequisites
\end_layout

\begin_layout Standard
The whole training pipeline is written in Python 3, using pytorch and torchvisio
n for defining the network architecture, preprocessing the data and finally
 train the model.
 Also, NumPy was used for some matrix handling outside pytorch, matplotlib
 for plotting the results and tqdm for nicer verbosity.
\end_layout

\begin_layout Standard
The training process was running on a GeForce GTX 1070 and a Intel Core
 i5-3570K, operating via Arch Linux.
\end_layout

\begin_layout Subsection
Output + Metrics
\end_layout

\begin_layout Standard
Since all posters in out dataset contain between 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula $7$
\end_inset

 labels, that have no order or weight, why we could not simplify the task
 to something easier than a multi-label classification problem.
 Therefore, our model outputs 
\begin_inset Formula $N=23$
\end_inset

 independent probabilities, each one representing how sure the model is,
 that the input (eventually among others) belongs to the respective genre.
\end_layout

\begin_layout Standard
To measure the performance of our model, two different metrics are used.
 First, we use a binary cross entropy loss, which is minimized during training.
 For a prediction vector 
\begin_inset Formula $p\in\left[0,1\right]^{N}$
\end_inset

 and a target vector 
\begin_inset Formula $t\in\left\{ 0,1\right\} ^{N}$
\end_inset

, the BCE loss is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L\left(x,y\right)=\frac{1}{N}\sum_{n=1}^{N}\left[t_{i}\cdot\log p_{i}+\left(1-t_{i}\right)\log\left(1-p_{i}\right)\right]\text{,}
\]

\end_inset

which rewards high probabilities on correct labels and penalizes high probabilit
ies on wrong labels.
\end_layout

\begin_layout Standard
To finally evaluate our models on the test set, we came up with an easy
 accuracy measure for multi-label classification like ours.
 If the 
\begin_inset Formula $M$
\end_inset

 labels assigned to a sample are in the Top
\begin_inset Formula $M$
\end_inset

 predictions (label-predictions with the highest probabilities), we want
 an accuracy of 
\begin_inset Formula $1$
\end_inset

.
 In contrast, we want an accuracy of 
\begin_inset Formula $0$
\end_inset

, if they are in the Bot
\begin_inset Formula $M$
\end_inset

(label-predictions with the worst probabilities).
 So we take every ground truth label separately, score it with 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

 in those two cases and scale linear between them.
 Then, the score of all labels is averaged.
 Example:
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $p_{\Xi}=\left(\begin{array}{c}
0.7\\
0.2\\
0.1\\
0.9\\
0.5
\end{array}\right)\text{, }t_{\Xi}=\left(\begin{array}{c}
1\\
0\\
0\\
0\\
1
\end{array}\right)\text{, scores}_{\Xi}=\left[1,0.5\right]\text{, acc}=0.75$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $p_{\heartsuit}=\left(\begin{array}{c}
0.9\\
0.1\\
0.5\\
0.8\\
0.2
\end{array}\right)\text{, }t_{\heartsuit}=\left(\begin{array}{c}
0\\
0\\
0\\
0\\
1
\end{array}\right)\text{, scores}_{\heartsuit}=\left[0.25\right]\text{, acc}=0.25$
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
In the 
\begin_inset Formula $\Xi$
\end_inset

-example, two labels are assigned, class 
\begin_inset Formula $0$
\end_inset

 and class 
\begin_inset Formula $4$
\end_inset

.
 Class 
\begin_inset Formula $0$
\end_inset

 is rated 2nd (which is in the Top
\begin_inset Formula $2$
\end_inset

), therefore it has an accuracy of 
\begin_inset Formula $1$
\end_inset

.
 Class 
\begin_inset Formula $4$
\end_inset

 is rated 3rd and therefore exactly in the middle of the linear scaled area.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\heartsuit$
\end_inset

-example shows the linear scaling better.
 Since there is only one label, the 2nd, 3rd and 4th predictions score 
\begin_inset Formula $0.75$
\end_inset

, 
\begin_inset Formula $0.5$
\end_inset

 and 
\begin_inset Formula $0.25$
\end_inset

.
\end_layout

\begin_layout Subsection
Approaches
\end_layout

\begin_layout Standard
On the way of finding 
\end_layout

\begin_layout Subsubsection
Small Network
\end_layout

\begin_layout Subsubsection
Smaller Network
\end_layout

\begin_layout Subsubsection
Small Network (random crop)
\end_layout

\begin_layout Subsubsection
Medium Network (random crop)
\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\end_body
\end_document
